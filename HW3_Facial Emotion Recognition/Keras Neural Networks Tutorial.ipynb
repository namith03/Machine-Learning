{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Keras Neural Networks Tutorial.ipynb","provenance":[{"file_id":"1VFMflojNcoP_MjFcV9-ARTydQQuX-nqp","timestamp":1584120325959}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mG4lsoXbAGeN"},"source":["# **Setting up GPU**\n","\n","*   Go to **Edit** -> **Notebook Settings** -> **Hardware Accelerator**\n","*   Select GPU\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZcqiwlNYAh7B"},"source":["# **Keras**\n","\n","\n","*   A deep learning framework that offers simple **APIs to implement and train common neural network architectures**.\n","*   Integrated with low-level deep learning library, tensorflow.\n","*   Provides flexibility to define models using both keras and tensorflow simultaneously.\n","*   Read more: https://keras.io/why-use-keras/\n"]},{"cell_type":"markdown","metadata":{"id":"baKCMztVXuyU"},"source":["# **Training a Neural Network Classifier** \n","\n","\n","1.   Load and Process Data\n","2.   Define Model\n","3.   Compile and Train\n","4.   Evaluate\n"]},{"cell_type":"markdown","metadata":{"id":"h2WdsnWwb_n4"},"source":["# **Load Dataset**\n","\n","*   We will work with **MNIST, Fashion-MNIST and IMDB**  datasets\n","*   Keras includes some common datasets (all three are there)\n","\n","**MNIST** \n","*   Database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples\n","*   Each digit is represented 28 x 28 pixel values\n","*   Digit has to be classified as one of the 0-9 (Total 10 classes)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"xQfw1UIEcXne","colab":{"base_uri":"https://localhost:8080/","height":401},"executionInfo":{"status":"ok","timestamp":1634852628513,"user_tz":300,"elapsed":4555,"user":{"displayName":"Eric Yeh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09299363538321051023"}},"outputId":"1ab194aa-f5a6-472e-8858-8fa4562973a1"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from tensorflow.keras.datasets import mnist\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tensorflow.keras.utils import to_categorical\n","\n","\n"," #Load MNISt dataset\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","# Check number of samples (60000 in training and 10000 in test)\n","# Each image has 28 x 28 pixels\n","print(\"Train Image Shape: \", train_images.shape, \"Train Label Shape: \", train_labels.shape) \n","print(type(train_images))\n","print(\"Test Image Shape: \", test_images.shape, \"Test Label Shape: \", test_labels.shape) \n","\n","print(train_labels)\n","#  Visualizing a random image (11th) from training dataset\n","print(\"Visualizing a random image (11th) from training dataset\")\n","_ = plt.imshow(train_images[10])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","Train Image Shape:  (60000, 28, 28) Train Label Shape:  (60000,)\n","<class 'numpy.ndarray'>\n","Test Image Shape:  (10000, 28, 28) Test Label Shape:  (10000,)\n","[5 0 4 ... 5 6 8]\n","Visualizing a random image (11th) from training dataset\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"iGQc3S6x-VHc"},"source":["# Preprocessing: Normalize the images.\n","train_images = (train_images / 255) - 0.5\n","test_images = (test_images / 255) - 0.5\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5RgHWBDzVm5"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"TBv3vs2UcCv1"},"source":["# **Define Neural Network Model**\n","\n","To define simple models, we will use\n","\n","*   **Sequential Model** which is  simply linear stack of layers \n","*   **Layers** include Linear layer (**Dense**), Convolutional Layers (**Conv1D, Conv2D etc.**), recurrent layers (**RNN, LSTM**), dropout etc.\n","\n","\n","In more complicated scenarios (e.g. Multi-output model, models with shared parameters etc.), we will use **Functional APIs**.\n","*    This will be used in later part of tutorial, where we implement a multi-task learning system.\n"]},{"cell_type":"markdown","metadata":{"id":"4oLEvU_uVkpP"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1EgtcNx9NTJD"},"source":["\n","# **Feed-Forward Neural Network**\n","\n","*   For a simple feed-forward neural network, only use stack of **Dense** and **Dropout** Layers\n","\n","*   **Dense** implements the operation: $output=activation(W^Tx + b)$\n","\n","*   **Dropout** is the most commonly (**almost 100%**) used mechanism to avoid overfitting in a neural network. It randomly set a fraction (rate) of input units to **zero** during training. \n","\n","*    Regularization techniques such as $l_1$ and $l_2$ norms are too expensive when training a big neural network with millions of parameters (**extremely SLOW**).\n","\n","*    Use **Dropout without an exception**. You will see that the first model we define next has over 900K parameters that will be trained using just 60,000 samples. When using such **overparametrization**, model can easily remember labels for each training sample but fail to generalize to test samples.\n","*    It is a good practice to validate your model's architecture by using $model.summary()$ \n"]},{"cell_type":"code","metadata":{"id":"dCvIW55RcgMK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634851715891,"user_tz":300,"elapsed":5521,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"1b424f4a-26de-4cc8-a316-d097bdc7567e"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")  # Ignore some warning logs\n","\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","\n","#  Define a Feed-Forward Model with 2 hidden layers with dimensions 392 and 196 Neurons\n","model = Sequential([\n","  Dense(784, activation='relu', input_shape=(28*28,), name=\"first_hidden_layer\"),\n","  Dense(784//2, activation='relu', name=\"second_hidden_layer\"), Dropout(0.25),\n","  Dense(10, activation='softmax'),\n","])\n","\n","#  Validate your Model Architecture\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","first_hidden_layer (Dense)   (None, 784)               615440    \n","_________________________________________________________________\n","second_hidden_layer (Dense)  (None, 392)               307720    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 392)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 10)                3930      \n","=================================================================\n","Total params: 927,090\n","Trainable params: 927,090\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","metadata":{"id":"6QEnfLiecfPh"},"source":["# **Training our Neural Network**\n","\n","*   Before training our model, we need to configure the learning process by using $compile()$ method\n","\n","> We specify **three** training parameters:\n","\n","1.   **Optimizer**: e.g. $sgd$ (stochastic gradient descent) or some advance optimizers such as $adam$ (**very stable training**). For a simple model, while the difference in model's performance is not significant, when using very deep neural netoworks like RNNs/ LSTMs it becomes difficult to successfully train them with SGD.\n","2.   **Loss**: Objective that we want to optimize e.g. RSS, Cross-Entropy etc.\n","3.   **Metrics**: List of metrics you want to use to evaluate your model e.g. Accuracy, F1 score etc.\n","\n","\n","*   For training our models, we typically use $fit()$ method, pass training data and labels, specify other hyper-parameters such as $batch\\_size, epochs$.\n","\n"]},{"cell_type":"code","metadata":{"id":"UUOZyXoC2RbB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQG9pgaQ-vYF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634851737209,"user_tz":300,"elapsed":21322,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"fe7aab72-2eb4-4f12-8ac2-fe35355a8aad"},"source":["# Compile model\n","model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","# Flatten the images into vectors (1D) for feed forward network\n","flatten_train_images = train_images.reshape((-1, 28*28))\n","flatten_test_images = test_images.reshape((-1, 28*28))\n","print(\"Train image shape: \", train_images.shape, \"Flattened image shape: \", flatten_train_images.shape)\n","print(train_labels.shape)\n","\n","\n","print(type(flatten_train_images[0,0]))\n","print(type(train_labels[0]))\n","\n","# Train model\n","model.fit(flatten_train_images, to_categorical(train_labels), epochs=10, batch_size=256,)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train image shape:  (60000, 28, 28) Flattened image shape:  (60000, 784)\n","(60000,)\n","<class 'numpy.float64'>\n","<class 'numpy.uint8'>\n","Epoch 1/10\n","235/235 [==============================] - 4s 5ms/step - loss: 1.3522 - accuracy: 0.6551\n","Epoch 2/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.6473 - accuracy: 0.8317\n","Epoch 3/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.4949 - accuracy: 0.8625\n","Epoch 4/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.4322 - accuracy: 0.8763\n","Epoch 5/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.3950 - accuracy: 0.8870\n","Epoch 6/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.3701 - accuracy: 0.8918\n","Epoch 7/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.3506 - accuracy: 0.8977\n","Epoch 8/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.3353 - accuracy: 0.9018\n","Epoch 9/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.3234 - accuracy: 0.9061\n","Epoch 10/10\n","235/235 [==============================] - 1s 5ms/step - loss: 0.3127 - accuracy: 0.9091\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0f102f7890>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"cM3e2dtPcchl"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GV01bLL8AWcj"},"source":["# **Evaluation**\n","\n","*   Evaluate the model on test images using $model.evaluate()$, pass test images and labels as arguments. \n","\n"]},{"cell_type":"code","metadata":{"id":"yIW3Kz2QAVPJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634851738757,"user_tz":300,"elapsed":1551,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"a8d5c829-9f5b-4f7a-8eff-3444411f7916"},"source":["# Evaluate your model's performance on the test data\n","performance = model.evaluate(flatten_test_images, to_categorical(test_labels))\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 3ms/step - loss: 0.2721 - accuracy: 0.9235\n","Accuracy on Test samples: 0.9235000014305115\n"]}]},{"cell_type":"markdown","metadata":{"id":"B8yVAfZbcuKP"},"source":["\n","---\n"]},{"cell_type":"code","metadata":{"id":"AaQUVNm2P5YS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SyrYjbJH-gF4"},"source":["# **Convolutional Neural Network**\n","\n","*   For large input (like image of size 1024*1024), fully connected NN requires a lot of parameters\n","*   Input can also have local structures\n","*   Use convolutional operation\n","*   Define a convolutional layer using $Conv2D$ that creates a 2d convloutional layers, specify arguments such as the size of filters (32), kernel (3), activation (relu) etc.\n","\n","**Note**: In this part of code, we separate the feature layers from classifier layer to accomodate the later part of tutorial on **Fine-tuning**. An equivalent implementation for\n","```\n","common_features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n","classifier = [Dense(512, activation='relu'), Dense(10, activation='softmax'),]\n","\n","cnn_model = Sequential(common_features+classifier)\n","```\n","\n","**is ==>**\n","\n","```\n","cnn_model = Sequential([Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), \n","            Flatten(),Dense(512, activation='relu'), Dense(10, activation='softmax'),])\n","```\n","\n","> **Compile, train, test the model and compare**\n","\n","\n","1.   Number of Parameters (almost half the size of fully connected NN)\n","2.   Performance (significantly better than fully connected NN, due to the capability of CNN to model local receptive field)\n","\n"]},{"cell_type":"code","metadata":{"id":"-Rf2vrEjAwvj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634851819002,"user_tz":300,"elapsed":80248,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"ef2983e7-e99a-4192-9a1a-d6dfd9bcc4da"},"source":["from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D\n","\n","# Define 2 groups of layers: features layer (convolutions) and classification layer\n","common_features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n","classifier = [Dense(512, activation='relu'), Dense(10, activation='softmax'),]\n","\n","cnn_model = Sequential(common_features+classifier)\n","\n","print(cnn_model.summary())  # Compare number of parameteres against FFN\n","cnn_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","train_images_3d = train_images.reshape(60000,28,28,1)\n","test_images_3d = test_images.reshape(10000,28,28,1)\n","\n","cnn_model.fit(train_images_3d, to_categorical(train_labels), epochs=10, batch_size=256,)\n","performance = cnn_model.evaluate(test_images_3d, to_categorical(test_labels))\n","\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1024)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               524800    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 594,922\n","Trainable params: 594,922\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","235/235 [==============================] - 33s 22ms/step - loss: 2.2319 - accuracy: 0.3820\n","Epoch 2/10\n","235/235 [==============================] - 5s 21ms/step - loss: 0.7692 - accuracy: 0.8121\n","Epoch 3/10\n","235/235 [==============================] - 5s 21ms/step - loss: 0.2763 - accuracy: 0.9172\n","Epoch 4/10\n","235/235 [==============================] - 5s 21ms/step - loss: 0.1980 - accuracy: 0.9408\n","Epoch 5/10\n","235/235 [==============================] - 5s 22ms/step - loss: 0.1559 - accuracy: 0.9537\n","Epoch 6/10\n","235/235 [==============================] - 5s 22ms/step - loss: 0.1307 - accuracy: 0.9612\n","Epoch 7/10\n","235/235 [==============================] - 5s 22ms/step - loss: 0.1126 - accuracy: 0.9662\n","Epoch 8/10\n","235/235 [==============================] - 5s 21ms/step - loss: 0.0992 - accuracy: 0.9699\n","Epoch 9/10\n","235/235 [==============================] - 5s 21ms/step - loss: 0.0901 - accuracy: 0.9728\n","Epoch 10/10\n","235/235 [==============================] - 5s 22ms/step - loss: 0.0834 - accuracy: 0.9746\n","313/313 [==============================] - 2s 4ms/step - loss: 0.0724 - accuracy: 0.9775\n","Accuracy on Test samples: 0.9775000214576721\n"]}]},{"cell_type":"code","metadata":{"id":"pS6Doslt455E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634851862230,"user_tz":300,"elapsed":43237,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"a1081900-c38a-4407-a593-b46ad4c83817"},"source":["from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D\n","\n","\n","# Define 2 groups of layers: features layer (convolutions) and classification layer\n","common_features_1 = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n","classifier = [Dense(512, activation='relu'), Dense(10, activation='softmax'),]\n","\n","cnn_model_1 = Sequential(common_features_1+classifier)\n","\n","print(cnn_model_1.summary())  # Compare number of parameteres against FFN\n","cnn_model_1.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","train_images_3d = train_images.reshape(60000,28,28,1)\n","test_images_3d = test_images.reshape(10000,28,28,1)\n","\n","cnn_model_1.fit(train_images_3d, to_categorical(train_labels), epochs=10, batch_size=256,)\n","performance = cnn_model_1.evaluate(test_images_3d, to_categorical(test_labels))\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 1600)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 512)               819712    \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 843,658\n","Trainable params: 843,658\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","235/235 [==============================] - 3s 12ms/step - loss: 2.0977 - accuracy: 0.5016\n","Epoch 2/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.8073 - accuracy: 0.8240\n","Epoch 3/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.3842 - accuracy: 0.8908\n","Epoch 4/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.2992 - accuracy: 0.9119\n","Epoch 5/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.2524 - accuracy: 0.9254\n","Epoch 6/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.2182 - accuracy: 0.9362\n","Epoch 7/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.1925 - accuracy: 0.9445\n","Epoch 8/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.1715 - accuracy: 0.9503\n","Epoch 9/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.1543 - accuracy: 0.9561\n","Epoch 10/10\n","235/235 [==============================] - 3s 12ms/step - loss: 0.1405 - accuracy: 0.9596\n","313/313 [==============================] - 1s 4ms/step - loss: 0.1272 - accuracy: 0.9605\n","Accuracy on Test samples: 0.9605000019073486\n"]}]},{"cell_type":"markdown","metadata":{"id":"QAhk91Hlween"},"source":["# Training Time\n","\n","\n","*   Previous results suggest using deeper models will increase the training time.\n","*   In general, assuming that we've got enough power for parallel computation, traning deeper networks takes longer compared to wider networks. This is possible thanks to the possibility of doing each layer's calcualtions in parallel. In other words, in a deep model, we cannot do the computaion of different layers simultaneously. \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u24HvB1vXpwI"},"source":["# Effective receptive field of convolutinal filters\n","\n","\n","*   Stacking two 3\\*3 conv layers = A single 5\\*5 filter \n","*   Stacking three 3\\*3 conv layers = A single 7\\*7 filter\n","\n","Note the change in the number of parameters:\n","\n","*   2 * (3\\*3) = 6 vs. 5\\*5 = 25\n","*   3 * (3\\*3) = 27 vs. 7\\*7 = 49\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oLb8n6YsDDIV"},"source":["\n","\n","# Transfer Learning (CNN Layers) and fine-tuning\n","\n","*   The Feature Layers (convolutional layers) learn some low-level patterns that can be effectively used for many (most) other tasks\n","*   These low level features are transferable\n","*   We use the **feature layers** from the pretrained model on MNIST dataset and disable parameter update for them\n","*   **Fine-tune classification layers** in the model on a new **Fashion MNIST** dataset\n","\n","**Fashion MNIST** \n","*   Database of fashion categories, has a training set of 60,000 examples, and a test set of 10,000 examples\n","*   Each image is represented 28 x 28 pixel values\n","*   Digit has to be classified as one of the 10 types\n","\n","**Categories:** \n","\n","0.   T-shirt/top\n","1.   Trouser\n","2.   Pullover\n","3.   Dress\n","4.   Coat\n","5.   Sandal\n","6.   Shirt\n","7.   Sneaker\n","8.   Bag\n","9.   Ankle Boot"]},{"cell_type":"code","metadata":{"id":"MOazkDftqksp","colab":{"base_uri":"https://localhost:8080/","height":508},"executionInfo":{"status":"ok","timestamp":1634851863724,"user_tz":300,"elapsed":1501,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"d9c70886-9a88-4fef-98c9-05e175676601"},"source":["from tensorflow.keras.datasets import fashion_mnist\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import show\n","\n","(train_fashion_images, train_fashion_labels), (test_fashion_images, test_fashion_labels) = fashion_mnist.load_data()\n","print(train_fashion_images.shape)\n","\n","train_fashion_images = (train_fashion_images / 255) - 0.5\n","test_fashion_images = (test_fashion_images / 255) - 0.5\n","print(\"Visualize a sample\")\n","_ = plt.imshow(train_fashion_images[7])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","40960/29515 [=========================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","26435584/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","16384/5148 [===============================================================================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","4431872/4422102 [==============================] - 0s 0us/step\n","(60000, 28, 28)\n","Visualize a sample\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"GMVSZpnGsXkQ"},"source":["\n","\n","---\n","\n","\n","*   **Freeze the Feature Layers** by disabling \"trainable\" attribute\n","\n","```\n","for l in common_features:\n","  l.trainable = False\n","```\n","\n","*    Check number of trainable parameters in model's summary\n","*    Feature Layers (Convolutional) are non-trainable\n","\n","```\n","Total params: 659,914\n","Trainable params: 594,922\n","Non-trainable params: 64,992\n","```\n"]},{"cell_type":"code","metadata":{"id":"xclXz00wUklJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634851868333,"user_tz":300,"elapsed":4613,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"757a7c5c-e81b-404c-8889-a11772250963"},"source":["train_fashion_images_3d = train_fashion_images.reshape(60000,28,28,1)\n","test_fashion_images_3d = test_fashion_images.reshape(10000,28,28,1)\n","\n","\n","for l in common_features:\n","  l.trainable = False\n","\n","print(cnn_model.summary())\n","\n","cnn_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","cnn_model.fit(train_fashion_images_3d, to_categorical(train_fashion_labels), epochs=1, batch_size=256,)\n","performance = cnn_model.evaluate(test_fashion_images_3d, to_categorical(test_fashion_labels))\n","\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1024)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               524800    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 594,922\n","Trainable params: 529,930\n","Non-trainable params: 64,992\n","_________________________________________________________________\n","None\n","235/235 [==============================] - 3s 10ms/step - loss: 0.9743 - accuracy: 0.6950\n","313/313 [==============================] - 2s 5ms/step - loss: 0.6681 - accuracy: 0.7383\n","Accuracy on Test samples: 0.7383000254631042\n"]}]},{"cell_type":"markdown","metadata":{"id":"vVzbFJtxFcwB"},"source":["\n","\n","---\n","\n","\n","**Compare Transferred Model against the Model trained from scratch**\n","\n","*   The performance is roughly 15-30% lower (depends on network initialization)\n","\n"]},{"cell_type":"code","metadata":{"id":"9nSIWWT-Feub","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634851876178,"user_tz":300,"elapsed":7849,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"d46a2527-36bc-4688-a751-a5163d62b36b"},"source":["features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n","classifier = [Dense(512, activation='relu'), Dense(10, activation='softmax'),]\n","\n","new_model = Sequential(features+classifier)\n","print(new_model.summary())\n","\n","new_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n","\n","new_model.fit(train_fashion_images_3d, to_categorical(train_fashion_labels), epochs=1, batch_size=256,)\n","performance = new_model.evaluate(test_fashion_images_3d, to_categorical(test_fashion_labels))\n","\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 24, 24, 32)        9248      \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 10, 10, 64)        18496     \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 8, 8, 64)          36928     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 4, 4, 64)          0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 512)               524800    \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 594,922\n","Trainable params: 594,922\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","235/235 [==============================] - 6s 21ms/step - loss: 1.9033 - accuracy: 0.3338\n","313/313 [==============================] - 1s 4ms/step - loss: 1.2544 - accuracy: 0.5483\n","Accuracy on Test samples: 0.54830002784729\n"]}]},{"cell_type":"markdown","metadata":{"id":"Hb3PqVrGwzbz"},"source":["# Multi-Task Learning\n","\n","\n","*   We observed feature layers are transferable across classification tasks (Transfer Learning)\n","*   Learned Fetaure layers can **generalize better** if we learn them **over multiple tasks and datasets** (Multi-task Learning)\n","*   In this part of tutorial, we will use **functions APIs** that provide greater flexibility to define complex models (e.g. two models on two different tasks with shared parameters)\n","*    Key Idea:\n","> Successively call Layers over the Input to get the Output\n","```\n","y = Layer_Ouput(Layer_Hidden(Layer_Input(x)))\n","```\n","> Use input and output to define Model \n","```\n","model = Model(inputs=train_X, outputs=train_y)\n","```\n","\n","*    We can **reuse** these trained layers in any new model by calling them on new input tensors\n","*    Training this model for 2 epochs gives superior performance against the individual models trained for 5-10 epochs\n","*    Model can quickly learn better generalizable features, if the objective tasks are related "]},{"cell_type":"code","metadata":{"id":"ReGtCWMnw2_4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"68413098-6527-4d32-8b9e-ae2be5abf466"},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","\n","# Define a fetaure extraction model that is shared for both mnist and fashion-mnist tasks\n","Base_feature_model = Sequential([Conv2D(32, kernel_size=3, activation='relu'), \n","            Conv2D(32, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Conv2D(64, kernel_size=3, activation='relu'),\n","            Conv2D(64, kernel_size=3, activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)),\n","            Flatten(), Dense(512, activation='relu'),])\n","\n","Classifier_mnist = Sequential([Dense(10, activation='softmax')])\n","Classifier_fashion_mnist = Sequential([Dense(10, activation='softmax')])\n","\n","# Instantiate a Tensor to feed Input (Input Layer)\n","mnist_input = Input(shape=(28,28,1))\n","fashion_mnist_input = Input(shape=(28,28,1))\n","\n","# Call Base_feature_model over the mnist images\n","mnist_features = Base_feature_model(mnist_input)\n","\n","# Call Base_feature_model over the fashion-mnist images\n","fashion_mnist_features = Base_feature_model(fashion_mnist_input)\n","\n","# Call mnist_prediction layer over the mnist images\n","# mnist_prediction represents the predicted output for mnist dataset\n","mnist_prediction = Classifier_mnist(mnist_features)\n","\n","# Call fashion_mnist_prediction layer over the mnist images\n","# fashion_mnist_prediction represents the predicted output for fashion-mnist dataset\n","fashion_mnist_prediction = Classifier_fashion_mnist(fashion_mnist_features)\n","\n","# define model by calling Model(inputs, outputs) on the instance of input layers and output layers\n","joint_model = Model(inputs=[mnist_input, fashion_mnist_input], \n","                    outputs=[mnist_prediction, fashion_mnist_prediction])\n","\n","print(joint_model.summary())\n","\n","joint_model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'],)  # Using adam optimizer for faster convergence\n","\n","joint_model.fit([train_images_3d, train_fashion_images_3d], \n","                [to_categorical(train_labels), to_categorical(train_fashion_labels)], \n","                epochs=1, batch_size=1024,)\n","performance = joint_model.evaluate([test_images_3d, test_fashion_images_3d], \n","                                   [to_categorical(test_labels), \n","                                    to_categorical(test_fashion_labels)], verbose=1)\n","\n","print(\"===\\nMNIST Accuracy: {0}\\nFashion MNIST Accuracy: {1}\".format(performance[3], performance[4]))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n","__________________________________________________________________________________________________\n","sequential_4 (Sequential)       (None, 512)          589792      input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","sequential_5 (Sequential)       (None, 10)           5130        sequential_4[0][0]               \n","__________________________________________________________________________________________________\n","sequential_6 (Sequential)       (None, 10)           5130        sequential_4[1][0]               \n","==================================================================================================\n","Total params: 600,052\n","Trainable params: 600,052\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n","59/59 [==============================] - 11s 150ms/step - loss: 0.5995 - sequential_5_loss: 0.2967 - sequential_6_loss: 0.3028 - sequential_5_accuracy: 0.4396 - sequential_6_accuracy: 0.4053\n"]}]},{"cell_type":"markdown","metadata":{"id":"TK-PNN1lq8bW"},"source":["# **Sequence Classification using Recurrent Neural Networks (LSTM)**\n","\n","*   In previous experiments, we observed CNN models are more suitable for image classification tasks (compared against feed-forward neural networks)\n","*   Choosing appropriate **Model Prior** is crucial to training a neural network model (Each model has innate capacity to capture certain aspects in the input data)\n","*   For modeling **sequential data** (e.g. time-series data, natural languages), recurrent neural network (a powerful version LSTM) provides a very strong prior\n","*   In this part, we will use IMDB corpus (movie reviews) to perform sentiment analysis\n","\n","**IMDB** \n","*   Database of 25,000 movie reviews each for training and test, labeled with positive/negative sentiments\n","*   Review is pre-processed and words are encoded with word-indices in a dictionary \n"]},{"cell_type":"code","metadata":{"id":"zCdA7D_rrAap"},"source":["from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing import sequence\n","\n","# select 5000 most frequent words, other infrequent words are replaced with <UNKNOWN> symbol\n","max_num_words = 5000\n","\n","max_sequence_length = 128\n","(train_imdb_texts, train_imdb_labels), (test_imdb_texts, test_imdb_labels) = imdb.load_data(num_words=max_num_words)\n","\n","# Since each review can have different number of words, we fix the input length to \"max_sequence_length=128\".\n","# Reviews with < 128 words will be padded with some symbol <PAD> and > 128 words will be truncated \n","train_imdb_texts = sequence.pad_sequences(train_imdb_texts, maxlen=max_sequence_length)\n","test_imdb_texts = sequence.pad_sequences(test_imdb_texts, maxlen=max_sequence_length)\n","\n","#  Print an example dataset (represented with word indices)\n","print(train_imdb_texts[0])\n","\n","#  Print actual review by mapping word indices to the corresponding word in the dictionary \n","index = imdb.get_word_index()\n","index_to_word = dict([value, key] for (key, value) in index.items())\n","\n","# Print Decoded Sequence\n","print(\"Text: \", ' '.join([index_to_word.get(i-3, \"<UNKNOWN>\") for i in train_imdb_texts[0]]))\n","print(\"Label: \", train_imdb_labels[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MN0M2Ax_sm1V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"882ef5fe-6e50-42f1-c26d-d95f566092e4"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# tensorflow 2.0+ will defaul use eager mode and will cause error for the code below\n","import tensorflow as tf\n","tf.compat.v1.disable_eager_execution()\n","\n","# Embedding layers transform word indices to continuous word vectors\n","# Apply LSTM on the sequence of word vectors\n","sentiment_classification_model = Sequential([Embedding(max_sequence_length, 256), \n","                                             LSTM(256, dropout=0.2, recurrent_dropout=0.2), \n","                                             Dense(1, activation='sigmoid')])\n","\n","print(sentiment_classification_model.summary())\n","\n","sentiment_classification_model.compile(optimizer='adam', loss='binary_crossentropy',\n","                                       metrics=['accuracy'],)  # Using adam optimizer for faster convergence\n","\n","print(train_imdb_texts[0])\n","print(train_imdb_labels.shape)\n","sentiment_classification_model.fit(train_imdb_texts, train_imdb_labels, epochs=5, batch_size=1024)\n","performance = sentiment_classification_model.evaluate(test_imdb_texts, test_imdb_labels)\n","\n","print(\"Accuracy on Test samples: {0}\".format(performance[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 256)         32768     \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 256)               525312    \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 1)                 257       \n","=================================================================\n","Total params: 558,337\n","Trainable params: 558,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","[  12    8  316    8  106    5    4 2223    2   16  480   66 3785   33\n","    4  130   12   16   38  619    5   25  124   51   36  135   48   25\n"," 1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n","    2    8    4  107  117    2   15  256    4    2    7 3766    5  723\n","   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n","  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n","    2   18    4  226   22   21  134  476   26  480    5  144   30    2\n","   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n","   88   12   16  283    5   16 4472  113  103   32   15   16    2   19\n","  178   32]\n","(25000,)\n","Train on 25000 samples\n","Epoch 1/5\n","25000/25000 [==============================] - 20s 816us/sample - loss: 0.6771 - accuracy: 0.5663\n","Epoch 2/5\n","25000/25000 [==============================] - 20s 792us/sample - loss: 0.6499 - accuracy: 0.6323\n","Epoch 3/5\n","25000/25000 [==============================] - 20s 792us/sample - loss: 0.6003 - accuracy: 0.6799\n","Epoch 4/5\n","25000/25000 [==============================] - 20s 793us/sample - loss: 0.5551 - accuracy: 0.7160\n","Epoch 5/5\n","25000/25000 [==============================] - 20s 796us/sample - loss: 0.5421 - accuracy: 0.7268\n"]}]},{"cell_type":"code","metadata":{"id":"Qqvv_hiS8LsZ"},"source":["from tensorflow.keras import backend as K\n","\n","# embedding layers: shows the relation of data into a higher dimension\n","print(len(train_imdb_texts[0]))\n","get_1st_layer_output = K.function([sentiment_classification_model.layers[0].input],\n","                                  [sentiment_classification_model.layers[0].output])\n","layer_output = get_1st_layer_output([train_imdb_texts[0]])[0]\n","print(layer_output)\n","print(len(layer_output))\n","print(len(layer_output[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zXSY1YeSrXu7"},"source":["\n","\n","---\n","\n","\n","**Can you try building a Feed-forward neural network for sentiment analysis and compare number of parameters and performance against LSTM-based model?**"]},{"cell_type":"markdown","metadata":{"id":"fDU9i4nvFQWS"},"source":["# **Hyperparameter Tuning**\n","\n","*    We find Parameters of a Neural Network using some optimzation technique \n","*    Hyperparmeters are obtained through search\n","*    Generally, we use **grid search** to find optimal hyperparameters (**Brute-Force Approach**)\n","*    **TPE** (Tree Stuctured Parzen Estimator) is another popular approach (based on Gaussian Processes) to estimate hyperparameters\n","*    Here, we use **Hyperopt** library that provides an easy to use TPE based hyperparameter optimization algorithm \n","\n"]},{"cell_type":"code","metadata":{"id":"fjricP7johWL"},"source":["!pip install hyperopt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7QP79QM-Ly4"},"source":["# Steps to use hyperopt\n","\n","*   Define hyperparameter search space\n","```\n","space = {'conv_kernel_size': hp.choice('conv_kernel_size', [3, 5]), \n","'dropout_prob': hp.uniform('dropout_prob', 0.1, 0.35),\n","'optimizer': hp.choice('optimizer', ['Adam', 'sgd']),\n","}\n","```\n","Here we search among 2 values (3,5) for kernel size of convolutional layers, 2 possible values (SGD, Adam) for optimizers, and the dropout rate of a float number between 0.1 and 0.35 sampled uniformly.\n","\n","*   Define the **objective function** to optimize\n","1.    It takes hyperparamers as argument \n","2.    Define, Compile and Train model, calculate loss/ accuracy on evaluation dataset\n","3.    Return a dictionary object with keys \"**loss**\" (float-valued function that we want to minimize) and \"**status**\" (keys from hyperopt.STATUS_STRINGS, such as 'ok' for successful completion, and 'fail' in cases where the function turned out to be undefined)\n","4.   You can also return the model object and later use that for evaluation on test-data\n","\n","\n","*   Specify **search algorithm (TPE)** to use \n","```\n","algo=tpe.suggest\n","```\n","\n","*   Maximum number of hyperparameters to try \n","```\n","max_evals=25\n","```\n","\n","*   **Optional**: create and pass **trials** object as an argument. With trials object, we can inspect all of the return values that are calculated during the experiment (e.g. losses, statuses, model)\n","\n","*   For more details: check https://github.com/hyperopt/hyperopt/wiki/FMin\n"]},{"cell_type":"code","metadata":{"id":"QYl9XDaTnefS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634852411771,"user_tz":300,"elapsed":40770,"user":{"displayName":"Peiman Mohseni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04085359289862589891"}},"outputId":"c7b14717-48a3-4cd6-947a-2b7f1ee68ad8"},"source":["from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n","\n","\n","def optimize_cnn(hyperparameter):\n","  \n","  # Define model using hyperparameters \n","  cnn_model = Sequential([Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], activation='relu', input_shape=(28,28,1)), \n","            Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), Dropout(hyperparameter['dropout_prob']),\n","            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'),\n","            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'), \n","            MaxPooling2D(pool_size=(2,2)), Dropout(hyperparameter['dropout_prob']), \n","            Flatten(),\n","            Dense(512, activation='relu'), \n","            Dense(10, activation='softmax'),])\n","  \n","  cnn_model.compile(optimizer=hyperparameter['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'],)\n","\n","  # create a training (50K samples) and validation (10K samples) subsets from training images.\n","  # Validation subset will be used to find the optimal hyperparameters\n","  train_X, train_y = train_images_3d[:50000], train_labels[:50000]\n","  valid_X, valid_y = train_images_3d[50000:], train_labels[50000:]\n","\n","  _ = cnn_model.fit(train_X, to_categorical(train_y), epochs=2, batch_size=256, verbose=0)\n","  # Evaluate accuracy on validation data\n","  performance = cnn_model.evaluate(valid_X, to_categorical(valid_y), verbose=0)\n","\n","  print(\"Hyperparameters: \", hyperparameter, \"Accuracy: \", performance[1])\n","  print(\"----------------------------------------------------\")\n","  # We want to minimize loss i.e. negative of accuracy\n","  return({\"status\": STATUS_OK, \"loss\": -1*performance[1], \"model\":cnn_model})\n","  \n","\n","# Define search space for hyper-parameters\n","space = {\n","    # The kernel_size for convolutions:\n","    'conv_kernel_size': hp.choice('conv_kernel_size', [1, 3, 5]),\n","    # Uniform distribution in finding appropriate dropout values\n","    'dropout_prob': hp.uniform('dropout_prob', 0.1, 0.35),\n","    # Choice of optimizer \n","    'optimizer': hp.choice('optimizer', ['Adam', 'sgd']),\n","}\n","\n","trials = Trials()\n","\n","# Find the best hyperparameters\n","best = fmin(\n","        optimize_cnn,\n","        space,\n","        algo=tpe.suggest,\n","        trials=trials,\n","        max_evals=25,\n","    )\n","\n","print(\"==================================\")\n","print(\"Best Hyperparameters\", best)\n","\n","# You can retrain the final model with optimal hyperparameters on train+validation data\n","\n","# Or you can use the model returned directly\n","# Find trial which has minimum loss value and use that model to perform evaluation on the test data\n","test_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n","\n","performance = test_model.evaluate(test_images_3d, to_categorical(test_labels))\n","\n","print(\"==================================\")\n","print(\"Test Accuracy: \", performance[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.14376774571300735, 'optimizer': 'Adam'}\n","Accuracy: \n","0.9867\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 1, 'dropout_prob': 0.2057459635122148, 'optimizer': 'sgd'}\n","Accuracy: \n","0.2541\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.32100381326119465, 'optimizer': 'sgd'}\n","Accuracy: \n","0.6649\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 1, 'dropout_prob': 0.34182342570577506, 'optimizer': 'sgd'}\n","Accuracy: \n","0.1139\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.187654064407475, 'optimizer': 'Adam'}\n","Accuracy: \n","0.9867\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 5, 'dropout_prob': 0.33422027372086827, 'optimizer': 'Adam'}\n","Accuracy: \n","0.9859\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 5, 'dropout_prob': 0.16856837193402874, 'optimizer': 'Adam'}\n","Accuracy: \n","0.983\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 1, 'dropout_prob': 0.31397583447607846, 'optimizer': 'Adam'}\n","Accuracy: \n","0.9175\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 1, 'dropout_prob': 0.119248287046968, 'optimizer': 'Adam'}\n","Accuracy: \n","0.922\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 5, 'dropout_prob': 0.11728225117338945, 'optimizer': 'sgd'}\n","Accuracy: \n","0.3656\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 1, 'dropout_prob': 0.26771587914115913, 'optimizer': 'Adam'}\n","Accuracy: \n","0.9244\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.283093759979101, 'optimizer': 'Adam'}\n","Accuracy: \n","0.988\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 5, 'dropout_prob': 0.16663716471732265, 'optimizer': 'sgd'}\n","Accuracy: \n","0.2986\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.11504425552115738, 'optimizer': 'sgd'}\n","Accuracy: \n","0.7906\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.21803257226541917, 'optimizer': 'sgd'}\n","Accuracy: \n","0.7655\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 5, 'dropout_prob': 0.2346380937999273, 'optimizer': 'sgd'}\n","Accuracy: \n","0.1557\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 5, 'dropout_prob': 0.2139182628460139, 'optimizer': 'sgd'}\n","Accuracy: \n","0.2805\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.20557942673695795, 'optimizer': 'sgd'}\n","Accuracy: \n","0.6213\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.2106281079752636, 'optimizer': 'sgd'}\n","Accuracy: \n","0.7666\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.3395646062435111, 'optimizer': 'sgd'}\n","Accuracy: \n","0.776\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.268664026313218, 'optimizer': 'Adam'}\n","Accuracy: \n","0.986\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.28536325262757345, 'optimizer': 'Adam'}\n","Accuracy: \n","0.989\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.28882630053059016, 'optimizer': 'Adam'}\n","Accuracy: \n","0.9879\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.250034839827584, 'optimizer': 'Adam'}\n","Accuracy: \n","0.9877\n","----------------------------------------------------\n","Hyperparameters: \n","{'conv_kernel_size': 3, 'dropout_prob': 0.2968773194718934, 'optimizer': 'Adam'}\n","Accuracy: \n","0.9871\n","----------------------------------------------------\n","100%|| 25/25 [05:23<00:00, 12.96s/it, best loss: -0.9890000224113464]\n","==================================\n","Best Hyperparameters {'conv_kernel_size': 1, 'dropout_prob': 0.28536325262757345, 'optimizer': 0}\n","==================================\n","Test Accuracy:  0.989\n"]}]},{"cell_type":"code","metadata":{"id":"rW12wnRnTRhk"},"source":[""],"execution_count":null,"outputs":[]}]}